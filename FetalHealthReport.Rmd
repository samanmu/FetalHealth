---
title: "Choose Your Own project: 'FetalHealth'"
author: "Saman Musician"
date: "06/01/2021"
output: pdf_document
---

## Introduction
Reduction of child mortality is reflected in several of the United Nations' Sustainable Development Goals and is a key indicator of human progress.
The UN expects that by 2030, countries end preventable deaths of newborns and children under 5 years of age, with all countries aiming to reduce under 5 mortality to at least as low as 25 per 1,000 live births.
Parallel to notion of child mortality is of course maternal mortality, which accounts for 295000 deaths during and following pregnancy and childbirth (as of 2017). The vast majority of these deaths (94%) occurred in low-resource settings, and most could have been prevented.
In light of what was mentioned above, Cardiotocograms (CTGs) are a simple and cost accessible option to assess fetal health, allowing healthcare professionals to take action in order to prevent child and maternal mortality. The equipment itself works by sending ultrasound pulses and reading its response, thus shedding light on fetal heart rate (FHR), fetal movements, uterine contractions and more.
In this project, we use a data set, Fetal Health, including 2126 CTG results to perform a Machine Learning algorithm to determine the health condition of the fetal by the information provided from CTGs. The CTG results to features including fetal heart rate (baseline value), accelerations, fetal movement, uterine contraction, light, severe and prolonged decelerations (decelerations are an abrupt decrease in the baseline fetal heart rate of greater than 15 bpm for greater than 15 seconds), abnormal short term variability (Short term variation (STV) examines the variability of the fetal heart rate from beat to beat), mean value of short term variability, percentage of time with abnormal long term variability, mean value of long term variability and histogram information. For each CTG result there is a final classification of “fetal health” column in the data set, indicating normal (1), suspect (2) or pathological (3) situations.

## 1.1.	Goal of the project
In this project, we are implementing some algorithms learned in the Machine Learning course and the tools and instructions learned in Data Science program. The main goal is to comprehend the whole concept of a real project and its challenges. This project’s specific goal is to find and tune an algorithm to classify the health condition of CTG cases by the features provided with highest accuracy.

## 1.2.	Key Steps
First of all, the websites suggested in project overview was explored and a data set which seemed to be clean and useful for this project purpose was chosen. This data set is included in the Github repository for this project and is being read by the code. The original link to the data set is as follows:
https://www.kaggle.com/andrewmvd/fetal-health-classification/
The next step is the preprocessing step and data wrangling. We should transform the predictors if necessary and delete the columns which will not be used as predictors. Since the data is clean, the features columns which is going to be used will be selected. In this case all columns are being used as predictors. Then, we split the data into a train set which is used to optimize algorithms and a hold-out validation set for final validation of our algorithm. The analysis step is performed on the train set. K-nearest neighbors, decision tree and random forest algorithms are trained on the train set and tested on the test set in order to achieve the highest overall accuracy for our categorical classification project. The results are then presented and the performance of the models are discussed. Finally the conclusion is made on this project.

```{r include=FALSE}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(gridExtra)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(rpart)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(rpart.plot)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(matrixStats)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(randomForest)) install.packages("data.table", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(data.table)
library(gridExtra)
library(rpart)
library(rpart.plot)
library(matrixStats)
library(randomForest)
```

```{r echo=TRUE}
data <- read.csv("fetal_health.csv")
```

## 2. Analysis
In this section, I will explain the process and techniques used in order to reach our final algorithm.

## 2.1. Data Cleaning, Exploration and Visualization
Fortunately, the data used is cleaned by the provider and is ready to be used as inputs to the desired algorithms. Here is the summary of the data:
```{r echo=TRUE}
#summary(data)
colnames(data)
head(data)
```

As it is shown, the predictors have multiple scales of variations. All data features are included for training algorithms to classify the fetal health condition of the CTG cases randomly selected for testing. The fetal health classes are either normal (1), suspect (2) or pathological (3). We modify this outcome to be a factor class in our data set. Here is the probability of each fetal health condition in the data set.

```{r echo=FALSE}
data$fetal_health <- as.factor(data$fetal_health)
print("number of each health condition in CTG results:")
table(data$fetal_health)
print("health conditions Proportions:")
prop.table(table(data$fetal_health))
```

As it is shown 78 percent of the cases are normal, 14 percent are suspect and just about 8 percent are pathological.
In order to explore features, we plot each feature for different health conditions.

```{r include=FALSE}
p1 <- data %>% ggplot(aes(fetal_health,baseline.value)) + geom_boxplot()
p2 <- data %>% ggplot(aes(fetal_health,accelerations)) + geom_boxplot() #INSIGHT
p3 <- data %>% ggplot(aes(fetal_health,fetal_movement)) + geom_boxplot()
p4 <- data %>% ggplot(aes(fetal_health,uterine_contractions)) + geom_boxplot()
p5 <- data %>% ggplot(aes(fetal_health,light_decelerations)) + geom_boxplot()
p6 <- data %>% ggplot(aes(fetal_health,severe_decelerations)) + geom_boxplot()
p7 <- data %>% ggplot(aes(fetal_health,prolongued_decelerations)) + geom_boxplot()
p8 <- data %>% ggplot(aes(fetal_health,abnormal_short_term_variability)) + geom_boxplot()
p9 <- data %>% ggplot(aes(fetal_health,mean_value_of_short_term_variability)) + geom_boxplot()
p10 <- data %>% ggplot(aes(fetal_health,percentage_of_time_with_abnormal_long_term_variability)) + geom_boxplot()
p11 <- data %>% ggplot(aes(fetal_health,mean_value_of_long_term_variability)) + geom_boxplot() #INSIGHT
p12 <- data %>% ggplot(aes(fetal_health,histogram_width)) + geom_boxplot()
p13 <- data %>% ggplot(aes(fetal_health,histogram_min)) + geom_boxplot()
p14 <- data %>% ggplot(aes(fetal_health,histogram_max)) + geom_boxplot()
p15 <- data %>% ggplot(aes(fetal_health,histogram_number_of_peaks)) + geom_boxplot()
p16 <- data %>% ggplot(aes(fetal_health,histogram_number_of_zeroes)) + geom_boxplot()
p17 <- data %>% ggplot(aes(fetal_health,histogram_mode)) + geom_boxplot()
p18 <- data %>% ggplot(aes(fetal_health,histogram_mean)) + geom_boxplot()
p19 <- data %>% ggplot(aes(fetal_health,histogram_median)) + geom_boxplot() #INSIGHT
p20 <- data %>% ggplot(aes(fetal_health,histogram_variance)) + geom_boxplot() #INSIGHT
p21 <- data %>% ggplot(aes(fetal_health,histogram_tendency)) + geom_boxplot()

```

```{r echo=FALSE}
grid.arrange(p1,p2,p3,p4, ncol = 2)
grid.arrange(p5,p6,p7,p8, ncol = 2)
grid.arrange(p9,p10,p11,p12, ncol = 2)
grid.arrange(p13,p14,p15,p16, ncol = 2)
grid.arrange(p17,p18,p19,p20,p21, ncol = 2)

```

```{r include=FALSE}
rm(p1,p2,p3,p4,p5,p6,p7,p8,p9,p10,p11,p12,p13,p14,p15,p16,p17,p18,p19,p20,p21)
```

Exploring the predictors in this data set, we find out many features have impact on the fetal health but not necessarily in an obvious and distinctive way. For example, accelerations, mean value of long term variability, histogram mode, median and variance looks to have a decisive impact on the fetal health. However, a few features seem to have less impact on the fetal health classification. Although it seems better to remove “histogram number of zeroes” and “severe decelerations” features, I chose to keep all features available, and later evaluate their importance on the accuracy of our algorithms.

To start creating our models, we split a hold out validation set including 10% of the data. We use 90% of the data (called “fetalhealth” set) to train the algorithm and the “validation” set for our final test of the best trained algorithm.

```{r echo=TRUE, warning=FALSE}
set.seed(1, sample.kind = "Rounding" )
test_index <- createDataPartition(y = data$fetal_health, times = 1, p = 0.1, list = FALSE)
fetalhealth <- data[-test_index,]
validation <- data[test_index,]
rm(test_index)
dim(fetalhealth)
dim(validation)
```

In order to train using only the “fetalhealth” set, we split this data set in to a train set and a test set. The proportion of this train and test set is again 10% and 90%.

```{r echo=TRUE, warning=FALSE}
set.seed(1, sample.kind = "Rounding" )
index <- createDataPartition(fetalhealth$fetal_health, 1, p = 0.1, list = F)
train_set <- fetalhealth[-index,]
test_set <- fetalhealth[index,]
rm(index)
dim(train_set)
dim(test_set)
```

## 3.	Modeling Approaches
We try k-nearest neighbors, decision trees and random forest algorithms to find the best tune, and finally evaluate our optimized algorithm with the hold-out evaluation set.

## 3.1.	K-Nearest Neighbors:
For training a Knn algorithm, lots of calculations need to be done because it is needed to calculate the distance between many observations in train and test sets. Therefore, we use k-fold cross validation to improve the speed. Then we train a knn algorithm and tune the k in k-nearest neighbors. We divide the train set into a set of only predictors (x) and a set of the main label of each CTG case which is the fetal health class only (y). Then we use predict function to predict the fetal health class of the test set and calculate the accuracy of our model.

```{r echo=TRUE, warning=FALSE}
x <- train_set[,-22]    # 22 is the "fetal_health" column number
y <- train_set[,22]

set.seed(1, sample.kind = "Rounding")  #sample.kind=“Rounding"/"Rejection"
control <- trainControl(method = "cv", number = 3, p = .9)
fit_knn <- train(x,y,method = "knn",
                 tuneGrid = data.frame(k=seq(3,11,2)),
                                       trControl = control)
ggplot(fit_knn, highlight = TRUE)
fit_knn$bestTune
y_hat_knn <- predict(fit_knn, test_set)

confusionMatrix(predict(fit_knn,test_set), test_set$fetal_health)
confusionMatrix(predict(fit_knn,test_set), test_set$fetal_health)$overal["Accuracy"]
```
As shown, after tuning this model classifies the test set classes with 92.7% accuracy. This accuracy can still get improved

## 3.2.	Decision Tree:
We train is decision tree on the train set for classifying the fetal health of each case in the test set. Here I used “rpart.plot” package and function to plot the tree in a clear way.

```{r echo=TRUE}
fit_tree <- rpart(fetal_health ~ ., data = train_set)
rpart.plot(fit_tree, tweak = 1.2,
           digits = 2, box.palette = list("Greens","Blues","Reds"),
           shadow.col="gray", nn=F)
```

The 3 different fetal health classes are shown by different colors. By using the predict function, we predict fetal health classes and then compute the accuracy of this decision tree algorithm.
We also check the variables importance to verify our first assumptions of some of the features to be less influential.

```{r echo=TRUE}
#summary(fit_tree)
varImp(fit_tree)
y_hat_t <- predict(fit_tree, test_set)
```

```{r echo=TRUE}
y_hat_tree <- 0
for (i in 1:length(test_set$fetal_health)){
  y_hat_tree[i] <- which.max(y_hat_t[i,])
}
y_hat_tree <- factor(y_hat_tree)
confusionMatrix(y_hat_tree,test_set$fetal_health)$overal["Accuracy"]
```

It is clearly shown that 4 predictors have had no importance in training the decision tree model. They could have easily removed during data explorations.
This algorithm is highly interpretable and easy to visualize. The accuracy is also higher than knn algorithm. However, we should note that decision trees are not flexible. It is usually better to improve the prediction performance and reduce instability by averaging multiple decision trees.

## 3.3.	Random Forest:
After using the decision tree model, it is probable that we can achieve better classification results by using a random forest model. Similar to the Knn model training, we divide the train set into a set of only predictors (x) and only outcome (y). In order to increase the accuracy we increased the ntree parameter in the randomForest function to 2000.

```{r echo=TRUE}
x <- train_set[,-22]    # 22 is the "fetal_health" column number
y <- train_set[,22]

fit_rf <- randomForest(x, y, ntree = 2000)
plot(fit_rf)

imp <- importance(fit_rf)
imp

y_hat_rf <- predict(fit_rf, test_set)
confusionMatrix(y_hat_rf,test_set$fetal_health)$overal["Accuracy"]

```

We have calculated variable importance on the features and realized that by averaging many decision trees the interpretability is not as much as a single decision tree algorithm.
However, the accuracy of this model is improved significantly as anticipated. Our final model accuracy is 0.947 on the test set.

## 4.	Results
Now that we optimized our classification model to be a random forest model, we train a similar rainforest algorithm with 2000 trees using the whole “Fetalhealth” data set and test it on the “validation” set which we have held out for validating our final model.

```{r echo=TRUE}
data_x <- fetalhealth[,-22]
data_y <- fetalhealth[,22]

final_rf <- randomForest(data_x, data_y, ntree = 2000)
y_hat_final_rf <- predict(final_rf, validation)
confusionMatrix(y_hat_final_rf,validation$fetal_health)$overal["Accuracy"]
```

The final accuracy on the validation set is 0.939. It is a bit lower than the accuracy we had on the test set. However, the difference in the total accuracy of this final model and our best tuned model on the train set is not significant. So there has not been much overtraining during the procedure.

## 5.	Conclusion
We chose a data set of 21 predictors and an outcome for each entry, being a class of 3 different levels and tried to classify the data by predictors. Some conclusions are made on this project. First, data exploration is a very important part of this data science project. It could be clearly shown that not all the columns of the data are helpful in predictions and classifications. Second, optimizing K-nearest neighbors algorithm, we can overcome the curse of dimensionality by using k-fold cross validation and improve speed. Third, a decision tree algorithm is capable of understandably visualizing the Machine Learning procedure and is very comprehensible. Using random forest algorithm can improve the accuracy of decision trees, although it lowers the interpretability.

## Future Work
To better understand the accuracy of trained models on the training set we determine the wrongly classified cases by each of the three algorithms we trained.

```{r echo=TRUE}
which(y_hat_knn != test_set$fetal_health)
which(y_hat_tree != test_set$fetal_health)
which(y_hat_rf != test_set$fetal_health)
```

By exploring these wrongly classified cases we see some meaningful similarities and uniqueness between algorithms and their predictions. Some improvements can be implemented to aggregate the three algorithms to focus on the differences between them. It is also possible to explore these data set more technically and consult with medical experts to gain more insight on the characteristics of the features of our data set.
